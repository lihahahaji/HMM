# HMM

> 隐马尔可夫模型

HMM模型是**概率图模型的一种统计模型
HMM模型描述的就是由这些 隐状态序列(**实体标记**) **生成 可观测状态(**可读文本**)的过程。从可观察的参数中确定该过程的隐含参数

## 两个基本假设

1. **马尔科夫假设**

   > 隐状态只与前一个时刻的隐状态有关 
   >
   > 即：当前的状态只和前一状态有关

2. **观测独立性假设**

   >每个词只与该时刻的隐状态有关
   >
   >即：某个观测只和生成他的隐藏状态有关

## HMM 模型的参数

1. 转移概率矩阵 $A$

   描述系统从一个状态转移到另一个状态的概率。这些概率构成了状态之间的转移矩阵。


2. 发射概率矩阵 $B$  （由隐藏状态生成观测状态的矩阵）

   描述系统从特定状态生成观察符号的概率。这些概率构成了状态到观察的发射矩阵。

3. 初始隐藏状态概率 $\pi$

   描述系统在时间序列开始时处于各个状态的概率。

一个 HMM模型可以表示为 $\lambda = (\pi,A,B)$

------



1. 状态集（隐藏的，不可观测）

   这是模型中的一组离散的隐藏状态，代表系统在不同时刻的内部状态。在自然语言处理任务中，状态集可能表示词性。

2. 观测集（可观测的）

   这是在每个时刻模型可以观察到的符号，也就是可见的输出。在自然语言处理中，观测符号可能是单词。



> 关于观测状态和隐藏状态的举例：
>
> 海藻的状态有四种，分别是Dry（干燥的）、Dryish（稍干的）、Damp（潮湿的）、Soggy（湿漉漉的）。海藻的状态是可观测的，那它就是 **观测状态**，天气信息看不到就是 **隐藏状态**。



## HMM 的三类问题

- 概率计算问题：给定模型$\lambda=(\pi,A,B)$ 和观测序列 $O$ 的情况下，求在模型 $\lambda$ 下观测序列 $O = \{o_1,o_2,\dots,o_T\}$ 出现的概率$P(O|λ)$。（Forward-Backward算法）

  > 例如：计算𝑃(𝑥 =< You hate green bananas > | 𝐴, 𝐵)，即给定 HMM 的参数，计算可观测序列 的概率

- 解码问题：给定模型 $\lambda=(\pi,A,B)$ 和观测序列 $O = \{o_1,o_2,\dots,o_T\}$ 的情况下，求对给定观测序列$P(I | O)$ 最大的状态序列 $I$ 。（Viterbi算法）

  > 例如：输出句子𝑥 =< You hate green bananas >的解码结果，即给定 HMM 的参数和可观测序列，计算 tag 序列。

- 学习问题：观测序列 $O = \{o_1,o_2,\dots,o_T\}$ 已知的情况下，求解模型 $\lambda=(\pi,A,B)$ 参数，使得在该模型下观测序列概率 $P(O|λ)$ 最大。（极大似然估计算法）



> 例子：状态集 S ={+,−,𝑂}
>
> 观测集 O = {I,like,Fruits,You,bananas,hate,green}
>
> 初始概率：P(+) = 0.2 P(−) = 0.2 P(𝑂) = 0.6
>
> 转移概率矩阵A：
>
> A = \begin{bmatrix}
> 0.3 & 0.3 & 0.4 \\
> 0.2 & 0.5 & 0.3 \\
> 0.3 & 0.2 & 0.6
> \end{bmatrix}
>
>
> 发射概率矩阵B：
>
> B = \begin{bmatrix}
> 0.1 & 0.7 & 0.1 & 0.0 & 0.1 & 0.0 & 0.0 \\
> 0.0 & 0.0 & 0.0 & 0.2 & 0.0 & 0.7 & 0.1 \\
> 0.2 & 0.2 & 0.1 & 0.1 & 0.1 & 0.1 & 0.2
> \end{bmatrix}
> 假设马尔科夫假设和观测独立性假设都成立
> 求条件概率𝑃(𝑦=<+,+,+>|𝑥=<Ilikefruits>,𝐴,𝐵)

